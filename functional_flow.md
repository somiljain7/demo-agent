#  System Flow & Functionality

## Table of Contents
1. [System Overview](#system-overview)
2. [Component Architecture](#component-architecture)
3. [Data Flow Diagram](#data-flow-diagram)
4. [Detailed Component Flows](#detailed-component-flows)

---

## System Overview

This is a **LiveKit-based AI Voice Agent** system that enables real-time voice conversations with an AI assistant that has access to a knowledge base extracted from websites.

### Core Components
- **FastAPI Backend** (`main.py`) - REST API server
- **Agent Worker** (`agent.py`) - LiveKit voice agent with RAG capabilities
- **Web Scraper** (`sitemap.py`) - Website-to-markdown converter
- **Vector DB** (`vector_db_init.py`) - Knowledge base storage & retrieval

---

## Component Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        USER INTERFACE                        â”‚
â”‚                    (HTML Demo Page in FastAPI)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      FASTAPI BACKEND                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚   /token     â”‚  â”‚    /demo     â”‚  â”‚ /extract-kb  â”‚      â”‚
â”‚  â”‚   endpoint   â”‚  â”‚   endpoint   â”‚  â”‚   endpoint   â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                      â”‚
         â”‚                                      â–¼
         â”‚                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                          â”‚   SITEMAP SCRAPER    â”‚
         â”‚                          â”‚  (WebsiteToMD)       â”‚
         â”‚                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                      â”‚
         â”‚                                      â–¼
         â”‚                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                          â”‚   VECTOR DB INIT     â”‚
         â”‚                          â”‚  (Qdrant + Embeddings)â”‚
         â”‚                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      LIVEKIT ROOM                            â”‚
â”‚                    (Real-time Voice)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AGENT WORKER                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  Speech-to-  â”‚  â”‚     LLM      â”‚  â”‚  Text-to-    â”‚      â”‚
â”‚  â”‚     Text     â”‚  â”‚   (openAI)   â”‚  â”‚    Speech    â”‚      â”‚
â”‚  â”‚  (Deepgram)  â”‚  â”‚              â”‚  â”‚  (Cartesia)  â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                          â”‚                                   â”‚
â”‚                          â–¼                                   â”‚
â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚                  â”‚  RAG Lookup  â”‚                           â”‚
â”‚                  â”‚   Function   â”‚                           â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Data Flow Diagram

### Flow 1: Knowledge Base Extraction

```
User Input (Website URL)
        â”‚
        â–¼
POST /extract-knowledge-base
        â”‚
        â”œâ”€â–º WebsiteToMarkdownPipeline.run()
        â”‚         â”‚
        â”‚         â”œâ”€â–º Discover sitemaps (robots.txt)
        â”‚         â”‚
        â”‚         â”œâ”€â–º Extract all URLs from sitemaps
        â”‚         â”‚
        â”‚         â”œâ”€â–º For each URL:
        â”‚         â”‚     â”‚
        â”‚         â”‚     â”œâ”€â–º Fetch HTML content
        â”‚         â”‚     â”‚
        â”‚         â”‚     â”œâ”€â–º Parse with BeautifulSoup
        â”‚         â”‚     â”‚
        â”‚         â”‚     â”œâ”€â–º Clean & convert to Markdown
        â”‚         â”‚     â”‚
        â”‚         â”‚     â””â”€â–º Save to knowledge_base/
        â”‚         â”‚
        â”‚         â””â”€â–º Create INDEX.md
        â”‚
        â–¼
MarkdownToVectorDB.init()
        â”‚
        â”œâ”€â–º Read all .md files from knowledge_base/
        â”‚
        â”œâ”€â–º Split texts into chunks (1000 chars)
        â”‚
        â”œâ”€â–º Generate embeddings (SentenceTransformer)
        â”‚
        â”œâ”€â–º Create Qdrant collection
        â”‚
        â””â”€â–º Upload vectors to Qdrant
              â”‚
              â–¼
        Knowledge Base Ready âœ…
```

---

### Flow 2: Voice Conversation Setup

```
User clicks "Start Voice Call"
        â”‚
        â–¼
POST /token (roomName, userName)
        â”‚
        â”œâ”€â–º Generate unique participant identity
        â”‚
        â”œâ”€â–º Create LiveKit AccessToken
        â”‚     â”‚
        â”‚     â”œâ”€â–º Set identity & name
        â”‚     â”œâ”€â–º Set TTL (2 hours)
        â”‚     â””â”€â–º Grant permissions (publish, subscribe)
        â”‚
        â”œâ”€â–º Return JWT token + LiveKit URL
        â”‚
        â–¼
Client connects to LiveKit Room
        â”‚
        â”œâ”€â–º Enable microphone
        â”œâ”€â–º Setup audio visualizer
        â””â”€â–º Wait for agent
              â”‚
              â–¼
        Agent Worker spawns
              â”‚
              â”œâ”€â–º Initialize AgentSession
              â”‚     â”‚
              â”‚     â”œâ”€â–º STT: Deepgram Nova-3
              â”‚     â”œâ”€â–º LLM: OpenAI GPT-4.1-mini
              â”‚     â””â”€â–º TTS: Cartesia Sonic-2
              â”‚
              â”œâ”€â–º Connect to room
              â”‚
              â”œâ”€â–º Say greeting
              â”‚
              â””â”€â–º Listen for user speech
```

---

### Flow 3: Real-time Voice Interaction with RAG

```
User speaks into microphone
        â”‚
        â–¼
Audio stream â†’ LiveKit Room
        â”‚
        â–¼
Agent receives audio
        â”‚
        â”œâ”€â–º Deepgram STT converts to text
        â”‚
        â–¼
Text â†’ LLM (GPT-4.1-mini)
        â”‚
        â”œâ”€â–º LLM analyzes query
        â”‚
        â”œâ”€â–º Decides if RAG lookup needed
        â”‚
        â–¼
[If knowledge needed]
        â”‚
        â”œâ”€â–º Call rag_lookup() function
        â”‚         â”‚
        â”‚         â”œâ”€â–º Convert query to embedding
        â”‚         â”‚
        â”‚         â”œâ”€â–º Search Qdrant (cosine similarity)
        â”‚         â”‚
        â”‚         â”œâ”€â–º Retrieve top 5 relevant chunks
        â”‚         â”‚
        â”‚         â””â”€â–º Return formatted results
        â”‚
        â–¼
LLM generates response
        â”‚
        â”œâ”€â–º Incorporates RAG context
        â”‚
        â”œâ”€â–º Formats conversational answer
        â”‚
        â–¼
Response text â†’ Cartesia TTS
        â”‚
        â”œâ”€â–º Convert to natural speech
        â”‚
        â–¼
Audio stream â†’ User's speakers
        â”‚
        â–¼
User hears AI response ğŸ”Š
```

---

## Detailed Component Flows

### 1ï¸âƒ£ FastAPI Backend (`main.py`)

#### Endpoints

**`GET /`** - Health check & info
```python
Returns: Service status, components, available endpoints
```

**`POST /token`** - Generate LiveKit access token
```python
Input:
  - room_name: string
  - participant_name: string

Process:
  1. Generate unique identity (name + random hex)
  2. Create AccessToken with API credentials
  3. Set permissions (publish, subscribe, join)
  4. Generate JWT token

Output:
  - token: JWT string
  - url: LiveKit server URL
  - room_name: string
```

**`POST /extract-knowledge-base`** - Scrape website
```python
Input:
  - website_url: string
  - max_pages: int (default: 50)

Process:
  1. Run WebsiteToMarkdownPipeline in background thread
  2. Scrape website â†’ markdown files
  3. Initialize vector database
  4. Convert markdown â†’ embeddings â†’ Qdrant

Output:
  - status: "completed"
  - message: success message
  - output_dir: "knowledge_base"
```

**`GET /demo`** - Serve interactive HTML page

---

### 2ï¸âƒ£ Agent Worker (`agent.py`)

#### VoiceAssistant Class

```python
Inherits from: livekit.agents.Agent

Features:
  - System instructions for conversational AI
  - RAG lookup function tool
  - Integration with STT/LLM/TTS

@function_tool: rag_lookup(query, limit=5)
  â”‚
  â”œâ”€â–º Initialize MarkdownToVectorDB
  â”œâ”€â–º Search similar documents
  â”œâ”€â–º Format results with title + text
  â””â”€â–º Return to LLM as context
```

#### Entry Point Flow

```python
async def entrypoint(ctx: JobContext):
  â”‚
  â”œâ”€â–º Parse room metadata (optional configs)
  â”‚
  â”œâ”€â–º Create AgentSession
  â”‚     â”œâ”€â–º STT: "deepgram/nova-3-general"
  â”‚     â”œâ”€â–º LLM: "openai/gpt-4.1-mini"
  â”‚     â”œâ”€â–º TTS: "cartesia/sonic-2:..."
  â”‚     â””â”€â–º preemptive_generation: True
  â”‚
  â”œâ”€â–º Setup metrics collection
  â”‚
  â”œâ”€â–º Start session with VoiceAssistant
  â”‚     â””â”€â–º room_input_options: noise_cancellation
  â”‚
  â”œâ”€â–º Connect to room
  â”‚
  â””â”€â–º Say initial greeting
```

---

### 3ï¸âƒ£ Web Scraper (`sitemap.py`)

#### WebsiteToMarkdownPipeline Flow

```python
run(website_url, max_pages):
  â”‚
  â”œâ”€â–º discover_sitemaps(base_url)
  â”‚     â”œâ”€â–º Check robots.txt for sitemap URLs
  â”‚     â””â”€â–º Try common sitemap locations
  â”‚
  â”œâ”€â–º extract_all_urls(sitemap_url)
  â”‚     â”œâ”€â–º Recursively parse sitemap XML
  â”‚     â”œâ”€â–º Handle sitemap indexes
  â”‚     â””â”€â–º Collect all page URLs
  â”‚
  â”œâ”€â–º For each URL (up to max_pages):
  â”‚     â”‚
  â”‚     â”œâ”€â–º fetch_url() with rate limiting
  â”‚     â”‚
  â”‚     â”œâ”€â–º clean_html_to_markdown()
  â”‚     â”‚     â”œâ”€â–º Parse with BeautifulSoup
  â”‚     â”‚     â”œâ”€â–º Remove scripts, styles, nav, footer
  â”‚     â”‚     â”œâ”€â–º Find main content area
  â”‚     â”‚     â”œâ”€â–º Convert to markdown (html2text)
  â”‚     â”‚     â””â”€â–º Add metadata header
  â”‚     â”‚
  â”‚     â””â”€â–º save_markdown()
  â”‚           â”œâ”€â–º Convert URL to safe filename
  â”‚           â”œâ”€â–º Create directory structure
  â”‚           â””â”€â–º Write .md file
  â”‚
  â””â”€â–º create_index()
        â””â”€â–º Generate INDEX.md with all pages
```

---

### 4ï¸âƒ£ Vector Database (`vector_db_init.py`)

#### MarkdownToVectorDB Flow

```python
process_markdown_files():
  â”‚
  â”œâ”€â–º extract_all_markdown_texts()
  â”‚     â”œâ”€â–º Scan knowledge_base/ directory
  â”‚     â”œâ”€â–º Read all .md and .markdown files
  â”‚     â””â”€â–º Return dict {filepath: text}
  â”‚
  â”œâ”€â–º split_texts_into_chunks()
  â”‚     â”œâ”€â–º Use RecursiveCharacterTextSplitter
  â”‚     â”‚     â”œâ”€â–º chunk_size: 1000 characters
  â”‚     â”‚     â””â”€â–º chunk_overlap: 200 characters
  â”‚     â”‚
  â”‚     â””â”€â–º Create chunk documents with metadata:
  â”‚           â”œâ”€â–º text (content)
  â”‚           â”œâ”€â–º source (file path)
  â”‚           â”œâ”€â–º title (extracted from markdown)
  â”‚           â”œâ”€â–º chunk_id (unique ID)
  â”‚           â””â”€â–º char_count, word_count
  â”‚
  â”œâ”€â–º create_embeddings()
  â”‚     â”œâ”€â–º Load SentenceTransformer model
  â”‚     â”‚     â””â”€â–º "all-MiniLM-L6-v2" (384 dimensions)
  â”‚     â”‚
  â”‚     â”œâ”€â–º Process in batches (32 texts/batch)
  â”‚     â””â”€â–º Generate numpy embeddings
  â”‚
  â”œâ”€â–º setup_qdrant_collection()
  â”‚     â”œâ”€â–º Connect to Qdrant (localhost:6333)
  â”‚     â”œâ”€â–º Delete existing collection (if exists)
  â”‚     â””â”€â–º Create new collection with:
  â”‚           â”œâ”€â–º Vector size: 384
  â”‚           â””â”€â–º Distance metric: COSINE
  â”‚
  â””â”€â–º upload_to_qdrant()
        â”œâ”€â–º Create PointStruct for each chunk
        â”‚     â”œâ”€â–º id: sequential integer
        â”‚     â”œâ”€â–º vector: embedding array
        â”‚     â””â”€â–º payload: metadata dict
        â”‚
        â””â”€â–º Upload in batches (100 points/batch)
```

#### Search Flow

```python
search_similar(query, limit=5):
  â”‚
  â”œâ”€â–º Encode query with SentenceTransformer
  â”‚
  â”œâ”€â–º Search Qdrant with cosine similarity
  â”‚
  â””â”€â–º Return top results with:
        â”œâ”€â–º text (truncated to 200 chars)
        â”œâ”€â–º score (similarity score)
        â”œâ”€â–º source (original file)
        â”œâ”€â–º title (heading)
        â””â”€â–º chunk_id
```

---

## ğŸ”„ User Journey

### Scenario: User asks about a product

```
1. User opens demo page at http://localhost:8000/demo
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ“š Knowledge Base Extraction Section           â”‚
â”‚                                                  â”‚
â”‚  Website URL: [https://example.com        ]     â”‚
â”‚  Max Pages:   [50                        ]     â”‚
â”‚                                                  â”‚
â”‚  [ğŸ” Extract Knowledge Base]  â† User clicks     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
POST /extract-knowledge-base
  {
    "website_url": "https://example.com",
    "max_pages": 50
  }
        â”‚
        â”œâ”€â–º Status shows: "â³ Extracting knowledge base..."
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         BACKGROUND PROCESSING STARTS            â”‚
â”‚         (User can continue browsing)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â”œâ”€â–º WebsiteToMarkdownPipeline.run()
        â”‚     â”‚
        â”‚     â”œâ”€â–º ğŸ” Discovering sitemaps...
        â”‚     â”‚     â””â”€â–º Found sitemap.xml
        â”‚     â”‚
        â”‚     â”œâ”€â–º ğŸ“„ Extracting URLs...
        â”‚     â”‚     â””â”€â–º Found 150 pages (limiting to 50)
        â”‚     â”‚
        â”‚     â”œâ”€â–º ğŸŒ Scraping pages... (1/50)
        â”‚     â”‚     â”œâ”€â–º Fetching https://example.com/
        â”‚     â”‚     â”œâ”€â–º Converting to Markdown
        â”‚     â”‚     â””â”€â–º Saved: knowledge_base/index.md
        â”‚     â”‚
        â”‚     â”œâ”€â–º ğŸŒ Scraping pages... (2/50)
        â”‚     â”‚     â””â”€â–º Processing...
        â”‚     â”‚
        â”‚     â””â”€â–º ... continues for all 50 pages
        â”‚
        â–¼
vector_db_init.init()
        â”‚
        â”œâ”€â–º ğŸ“– Reading markdown files...
        â”‚     â””â”€â–º Found 50 .md files
        â”‚
        â”œâ”€â–º âœ‚ï¸ Splitting into chunks...
        â”‚     â””â”€â–º Created 342 chunks (1000 chars each)
        â”‚
        â”œâ”€â–º ğŸ§® Creating embeddings...
        â”‚     â”œâ”€â–º Loading SentenceTransformer model
        â”‚     â””â”€â–º Generated 342 embeddings (384-dim)
        â”‚
        â”œâ”€â–º ğŸ—„ï¸ Setting up Qdrant collection...
        â”‚     â””â”€â–º Collection "markdown_knowledge_base" ready
        â”‚
        â””â”€â–º â¬†ï¸ Uploading to Qdrant...
              â””â”€â–º Uploaded 342 vectors
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  âœ… Success Message Appears:                    â”‚
â”‚                                                  â”‚
â”‚  "Successfully extracted knowledge base from    â”‚
â”‚   https://example.com and pushed to vectorDB"  â”‚
â”‚                                                  â”‚
â”‚  Output saved to: knowledge_base                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

2. Connection Phase
   â”œâ”€â–º User opens demo page
   â”œâ”€â–º Enters room name & username
   â”œâ”€â–º Clicks "Start Voice Call"
   â”œâ”€â–º Gets LiveKit token
   â”œâ”€â–º Connects to room
   â””â”€â–º Agent joins and greets

3. Conversation Phase
   â”œâ”€â–º User: "What's your best IEM under $100?"
   â”‚
   â”œâ”€â–º STT transcribes to text
   â”‚
   â”œâ”€â–º LLM analyzes query
   â”‚     â””â”€â–º Determines RAG needed
   â”‚
   â”œâ”€â–º rag_lookup("best IEM under $100", limit=5)
   â”‚     â”œâ”€â–º Query embedding created
   â”‚     â”œâ”€â–º Qdrant searches vectors
   â”‚     â”œâ”€â–º Returns 5 relevant chunks about IEMs
   â”‚     â””â”€â–º Formatted context returned
   â”‚
   â”œâ”€â–º LLM generates answer using RAG context
   â”‚     â””â”€â–º "Based on our catalog, I'd recommend..."
   â”‚
   â”œâ”€â–º TTS converts to speech
   â”‚
   â””â”€â–º User hears natural response

4. Follow-up Questions
   â””â”€â–º Process repeats with conversation history
```

---

## ğŸ”§ Key Technologies

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Backend** | FastAPI | REST API server |
| **Voice Platform** | LiveKit | Real-time audio/video |
| **STT** | Deepgram Nova-3 | Speech-to-text |
| **LLM** | OpenAI GPT-4.1-mini | Conversational AI |
| **TTS** | Cartesia Sonic-2 | Text-to-speech |
| **Embeddings** | SentenceTransformer | Vector representation |
| **Vector DB** | Qdrant | Similarity search |
| **Scraping** | BeautifulSoup | HTML parsing |
| **Markdown** | html2text | HTML â†’ Markdown |
| **Text Splitting** | LangChain | Chunking strategy |

---

##  Key Design Patterns

1. **Microservices Architecture**: Separate FastAPI backend and Agent worker
3. **Function Calling**: LLM triggers `rag_lookup()` when needed
4. **Streaming Audio**: Real-time bidirectional voice communication
5. **Vector Similarity Search**: Semantic retrieval from knowledge base
6. **Metadata Enrichment**: Store source, title, and context with vectors
7. **Preemptive Generation**: Agent starts generating while user speaks
8. **Rate Limiting**: Controlled website scraping (1s delay)
9. **Noise Cancellation**: Background voice cancellation (BVC)
11. **Async Operations**: Non-blocking knowledge base extraction
